# Chatbot de IA para Menu do Restaurante

## Vis√£o Geral

Este projeto implementa um chatbot de IA totalmente no navegador que permite aos clientes fazer perguntas sobre os pratos do menu, ingredientes, alerg√™nicos e obter recomenda√ß√µes personalizadas.

## Funcionalidades

### ü§ñ Chatbot Inteligente
- **Processamento local**: Toda a IA roda no navegador do usu√°rio
- **Sem servidores**: N√£o requer backend ou APIs externas
- **Respostas contextuais**: Entende perguntas sobre pratos espec√≠ficos
- **Fallback inteligente**: Funciona mesmo quando o WebLLM n√£o est√° dispon√≠vel

### üîç Recursos de Busca
- **Busca por pratos espec√≠ficos**: "Fil√© ao Poivre", "Torta de Chocolate"
- **Busca por categorias**: "Menu Principal", "Sobremesas", "Bebidas"
- **Busca por restri√ß√µes**: "sem gl√∫ten", "vegetariano", "sem lactose"
- **Busca por pre√ßos**: "pratos mais baratos", "acess√≠veis"
- **Busca por ingredientes**: "o que tem no Fil√© ao Poivre"

### üí° Recomenda√ß√µes Inteligentes
- Pratos em destaque
- Op√ß√µes para restri√ß√µes alimentares
- Pratos por categoria
- Sugest√µes baseadas em prefer√™ncias

## Tecnologias Utilizadas

### WebLLM
- **Framework**: @mlc-ai/web-llm
- **Modelo**: Llama-2-7b-chat-q4f16_1
- **API**: Compat√≠vel com OpenAI API
- **Processamento**: Totalmente no navegador
- **Performance**: Otimizado para dispositivos m√≥veis

### React + TypeScript
- **Interface moderna**: Design responsivo e acess√≠vel
- **TypeScript**: Tipagem forte para melhor desenvolvimento
- **Hooks personalizados**: useWebLLM para gerenciamento de estado

### Tailwind CSS
- **Design system**: Componentes consistentes
- **Responsivo**: Funciona em mobile e desktop
- **Tema escuro**: Suporte a modo escuro

## Estrutura do Projeto

```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ AIChatbot.tsx          # Componente principal do chatbot
‚îÇ   ‚îú‚îÄ‚îÄ SearchBar.tsx          # Barra de busca com bot√£o do chatbot
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useWebLLM.ts           # Hook personalizado para WebLLM
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ webllm-config.ts       # Configura√ß√µes e utilit√°rios do WebLLM
‚îî‚îÄ‚îÄ ...
```

## Como Usar

### Para o Cliente
1. **Acesse o menu** do restaurante
2. **Clique no bot√£o de IA** (√≠cone de l√¢mpada) no canto inferior direito
3. **Fa√ßa perguntas** como:
   - "Quais s√£o os pratos mais populares?"
   - "Tem op√ß√µes sem gl√∫ten?"
   - "Quais s√£o os ingredientes do Fil√© ao Poivre?"
   - "Recomenda√ß√µes para vegetarianos?"

### Para o Desenvolvedor

#### Instala√ß√£o
```bash
npm install @mlc-ai/web-llm
```

#### Configura√ß√£o
O chatbot √© configurado automaticamente com:
- Detec√ß√£o de compatibilidade do navegador
- Fallback para dispositivos n√£o suportados
- Otimiza√ß√µes de performance

#### Personaliza√ß√£o
```typescript
// Em src/hooks/useWebLLM.ts
const createSystemPrompt = useCallback(() => {
  // Personalize o prompt do sistema aqui
  return `Voc√™ √© um assistente especializado no restaurante ${menuData?.name}...`;
}, [menuData]);
```

## Configura√ß√µes de Performance

### WebLLM
- **Modelo**: Llama-2-7b-chat-q4f16_1 (otimizado para performance)
- **Tokens m√°ximos**: 512
- **Temperatura**: 0.7 (criatividade balanceada)
- **API**: OpenAI-compatible completions
- **Engine**: MLCEngine para infer√™ncia otimizada

### Fallback
- **Busca inteligente**: Algoritmo de busca baseado em palavras-chave
- **Respostas contextuais**: Entende contexto das perguntas
- **Sugest√µes**: Oferece op√ß√µes relevantes

## Compatibilidade

### Navegadores Suportados
- ‚úÖ Chrome 90+
- ‚úÖ Firefox 88+
- ‚úÖ Safari 14+
- ‚úÖ Edge 90+

### Dispositivos
- ‚úÖ Desktop
- ‚úÖ Tablet
- ‚úÖ Mobile (Android/iOS)

### Requisitos
- WebAssembly habilitado
- 4GB+ RAM (recomendado)
- Conex√£o inicial para download do modelo

## Limita√ß√µes

### WebLLM
- **Download inicial**: ~2GB do modelo (uma vez)
- **Mem√≥ria**: Requer RAM suficiente para o modelo
- **Performance**: Pode ser lento em dispositivos antigos

### Fallback
- **Respostas limitadas**: Baseadas em regras predefinidas
- **Menos criativo**: Respostas mais diretas
- **Contexto limitado**: N√£o mant√©m conversa complexa

## Melhorias Futuras

### Funcionalidades
- [ ] Reconhecimento de voz
- [ ] Sugest√µes baseadas em hist√≥rico
- [ ] Integra√ß√£o com sistema de pedidos
- [ ] An√°lise de sentimentos

### Performance
- [ ] Modelos menores para mobile
- [ ] Cache mais inteligente
- [ ] Lazy loading de componentes
- [ ] Otimiza√ß√µes de WebAssembly

### UX/UI
- [ ] Anima√ß√µes mais fluidas
- [ ] Temas personaliz√°veis
- [ ] Acessibilidade melhorada
- [ ] Modo offline

## Troubleshooting

### Problemas Comuns

#### WebLLM n√£o carrega
```bash
# Verifique se o WebAssembly est√° habilitado
# No console do navegador:
typeof WebAssembly !== 'undefined'
```

#### Performance lenta
- Reduza o tamanho do modelo em `webllm-config.ts`
- Aumente o cache dispon√≠vel
- Verifique a mem√≥ria do dispositivo

#### Erros de rede
- O fallback funciona offline
- Verifique a conex√£o para download inicial
- Cache local mant√©m funcionalidade

## Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes. 